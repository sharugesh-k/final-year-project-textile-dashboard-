================================================================================
GOOGLE COLAB + GEMINI AI - ML MODEL TRAINING PROMPT
TEXTILE MILL MONITORING SYSTEM
================================================================================

COPY THE PROMPT BELOW AND PASTE IT IN GOOGLE COLAB WITH GEMINI AI:

================================================================================
START OF PROMPT
================================================================================

You are an expert Machine Learning Engineer and Data Scientist. I need you to help me build, train, and evaluate machine learning models for a Textile Mill Monitoring System. The system needs to predict:

1. **Production Risk Prediction** (Classification) - Predict whether a production line will face downtime risk (0 = No Risk, 1 = At Risk)
2. **Supply Chain Risk Prediction** (Classification) - Predict whether a supplier delivery will be delayed
3. **Efficiency Forecasting** (Regression) - Predict future production efficiency percentages

---

## DATASET 1: PRODUCTION DATA (production_data.csv)

**Columns:**
- id: Unique identifier
- timestamp: DateTime of the record
- machine_id: Machine identifier (M1, M2, M3)
- target_output: Expected production units
- actual_output: Actual production units
- speed_rpm: Machine speed in RPM (700-1000 range)
- downtime_minutes: Minutes of downtime (0-3 range)
- temperature_c: Machine temperature in Celsius (28-40 range)
- output_gap: target_output - actual_output
- efficiency: (actual_output / target_output) * 100

**Sample Data:**
id,timestamp,machine_id,target_output,actual_output,speed_rpm,downtime_minutes,temperature_c,output_gap,efficiency
9492,2025-12-12 09:41:13,M2,87,69,852,0.98,38.18,18,79.31
9491,2025-12-12 09:41:10,M1,82,71,949,2.38,33.88,11,86.58
9490,2025-12-12 09:41:07,M2,97,82,749,0.78,38.7,15,84.54

---

## DATASET 2: SUPPLIER DATA (supplier_data.csv)

**Columns:**
- id: Unique identifier
- timestamp: DateTime of the record
- supplier_id: Supplier identifier (S1, S2, S3)
- material_type: Type of material (Cotton, Yarn, Dyes)
- expected_delivery_date: Expected delivery date
- actual_delivery_date: Actual delivery date
- order_quantity: Quantity ordered
- received_quantity: Quantity received
- price_per_kg: Price per kilogram
- transportation_status: Status (arrived, delayed, in-transit)
- supply_risk: Risk label (On Time, Delayed)

**Sample Data:**
id,timestamp,supplier_id,material_type,expected_delivery_date,actual_delivery_date,order_quantity,received_quantity,price_per_kg,transportation_status,supply_risk
4093,2025-12-12,S3,Cotton,2025-12-15,2025-12-16,1210,1035,177.3,arrived,Delayed
4092,2025-12-12,S3,Cotton,2025-12-18,2025-12-19,1233,1138,184.84,delayed,Delayed

---

## DATASET 3: RISK ALERTS (risk_alerts.csv)

**Columns:**
- id: Unique identifier
- timestamp: DateTime
- risk_type: Type of risk (production, supplier)
- entity_id: Machine/Supplier ID (M1-M4, S1-S3)
- risk_score: Risk probability (0.0 to 1.0)
- risk_label: Binary label (0 = No Risk, 1 = At Risk)

**Sample Data:**
id,timestamp,risk_type,entity_id,risk_score,risk_label
1,2025-12-03,production,M3,0.00013,0
2,2025-12-03,production,M3,0.9997,1
3,2025-12-03,supplier,S2,1,1

---

## TASK: BUILD COMPLETE ML PIPELINE

Please generate complete Python code for Google Colab that does the following:

### STEP 1: DATA LOADING & PREPROCESSING
```python
# Upload files from local system
from google.colab import files
uploaded = files.upload()

# Load libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Load datasets
production_df = pd.read_csv('production_data_20251212.csv')
supplier_df = pd.read_csv('supplier_data_20251212.csv')
risk_alerts_df = pd.read_csv('risk_alerts_rows.csv')
```

### STEP 2: EXPLORATORY DATA ANALYSIS (EDA)
- Display dataset shapes and info
- Show statistical summaries
- Check for missing values
- Visualize distributions of key features
- Create correlation heatmaps
- Plot target variable distributions

### STEP 3: FEATURE ENGINEERING
For Production Data:
- Create 'risk_label' based on: efficiency < 75 = High Risk (1), else = Low Risk (0)
- Extract time features: hour, day_of_week
- Encode machine_id using LabelEncoder
- Create rolling statistics if applicable

# NEW: Derived Causal Features
- `thermal_stress` = (temperature_c - 30) * (speed_rpm / 1000)
- `load_factor` = target_output / speed_rpm
- `efficiency_gap` = 100 - efficiency

For Supplier Data:
- Calculate 'delay_days' = actual_delivery_date - expected_delivery_date
- Calculate 'quantity_gap' = order_quantity - received_quantity
- Create 'is_delayed' binary target: delay_days > 0 = 1, else = 0
- Encode categorical variables

### STEP 4: MODEL BUILDING - PRODUCTION RISK CLASSIFICATION

Build and compare these models for predicting production risk:

1. **Logistic Regression** (baseline)
2. **Random Forest Classifier**
3. **XGBoost Classifier** (primary model)
4. **Gradient Boosting Classifier**

Features to use:
- speed_rpm
- downtime_minutes
- temperature_c
- target_output
- machine_id (encoded)

Target: risk_label (1 = At Risk, 0 = Normal)

### STEP 5: MODEL BUILDING - SUPPLIER DELAY CLASSIFICATION

Build models for predicting supplier delays:

1. **Random Forest Classifier**
2. **XGBoost Classifier**

Features to use:
- supplier_id (encoded)
- material_type (encoded)
- order_quantity
- price_per_kg
- transportation_status (encoded)

Target: is_delayed (1 = Delayed, 0 = On Time)

### STEP 6: MODEL BUILDING - EFFICIENCY REGRESSION

Build regression models to predict production efficiency:

1. **Random Forest Regressor**
2. **XGBoost Regressor**
3. **Linear Regression** (baseline)

Features: speed_rpm, downtime_minutes, temperature_c, target_output
Target: efficiency (continuous 0-100)

### STEP 7: MODEL EVALUATION

For Classification Models:
- Accuracy Score
- Precision, Recall, F1-Score
- Confusion Matrix (with visualization)
- ROC-AUC Curve
- Feature Importance Plot

For Regression Models:
- RÂ² Score
- Mean Squared Error (MSE)
- Root Mean Squared Error (RMSE)
- Mean Absolute Error (MAE)
- Actual vs Predicted scatter plot

### STEP 8: HYPERPARAMETER TUNING

Use GridSearchCV or RandomizedSearchCV to tune the best performing model:

For XGBoost Classifier:
```python
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}
```

### STEP 9: SAVE TRAINED MODELS

```python
import joblib

# Save the best models
joblib.dump(production_risk_model, 'production_risk_xgb.pkl')
joblib.dump(supplier_delay_model, 'supplier_delay_xgb.pkl')
joblib.dump(efficiency_model, 'efficiency_regressor.pkl')
joblib.dump(scaler, 'feature_scaler.pkl')
joblib.dump(label_encoders, 'label_encoders.pkl')

# Download models to local system
files.download('production_risk_xgb.pkl')
files.download('supplier_delay_xgb.pkl')
files.download('efficiency_regressor.pkl')
```

### STEP 10: MODEL INFERENCE FUNCTION

Create a reusable inference function:

```python
def predict_risk(speed_rpm, downtime_minutes, temperature_c, target_output, machine_id):
    """
    Predict production risk for given parameters.
    Returns: risk_label (0 or 1), risk_probability, risk_level
    """
    # Your implementation here
    pass

def predict_supplier_delay(supplier_id, material_type, order_quantity, price_per_kg, transport_status):
    """
    Predict if a supplier order will be delayed.
    Returns: is_delayed (0 or 1), delay_probability
    """
    # Your implementation here
    pass
```

---

## ADDITIONAL REQUIREMENTS:

1. **Use professional code structure** with proper comments and docstrings
2. **Handle edge cases** and missing values gracefully
3. **Create publication-quality visualizations** using matplotlib/seaborn
4. **Print model performance summaries** in a clean, formatted way
5. **Include a final summary table** comparing all models
6. **Save visualization plots** as PNG files for download

---

## EXPECTED OUTPUT FORMAT:

After running all cells, I should see:
1. Dataset overview and EDA visualizations
2. Feature engineering summary
3. Model training progress
4. Comparison table of all models with metrics
5. Best model selection with reasoning
6. Saved model files ready for download
7. Sample predictions using the trained models

Please generate the complete, executable Python code for Google Colab now.

================================================================================
END OF PROMPT
================================================================================

INSTRUCTIONS FOR USE:
1. Open Google Colab (colab.research.google.com)
2. Create a new notebook
3. Enable Gemini AI assistant in Colab
4. Copy the prompt above (between START and END markers)
5. Paste it in the Gemini chat or a cell
6. Upload your CSV files when prompted
7. Run the generated code cells one by one

NOTES:
- Make sure to upload all 3 CSV files from the data folder
- The models will be saved as .pkl files for later use
- You can download the trained models to use in your dashboard

================================================================================
