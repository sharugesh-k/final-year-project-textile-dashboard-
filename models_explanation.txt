# üéì The Expert ML & Database Masterclass

Hello! I am your AI instructor for today. We have two exciting sessions:
1.  **Machine Learning Masterclass**: We'll dive into the brains of your AI system.
2.  **Database Architecture Pro-Course**: We'll explore the backbone where all your data lives.

---

# üß† Session 1: Machine Learning Concepts & Project Models

In this project, we are using **Machine Learning (ML)** to make intelligent predictions. Instead of writing fixed rules (like "if temperature > 40, give warning"), we *train* models on historical data so they learn patterns and make probablistic decisions.

## 1. Random Forest Classifier (The "Committee" Approach)

We use this for **Risk Prediction** (Classifying if something is "High Risk" or "Low Risk").

### üéì Concept: How it Works
Imagine you have a complex decision to make. Instead of asking just one expert, you ask 100 different experts.
*   **Decision Tree**: One expert. It asks a series of Yes/No questions to arrive at an answer. (e.g., "Is it hot?" -> "Yes" -> "Is speed high?" -> "Yes" -> "Risk!").
*   **Random Forest**: A whole group (forest) of these decision trees.
*   Each tree gets a slightly different subset of data and votes on the outcome.
*   **The Verdict**: The majority vote wins. This makes it very robust and less likely to make mistakes than a single tree.

### üõ†Ô∏è In Your Project:
We use Random Forest for two critical models:

#### A. Production Downtime Risk Model (`production_risk_rf_model.pkl`)
*   **Goal**: Predict if a machine is about to fail or have high downtime.
*   **The "Questions" (Features) the model asks:**
    1.  `speed_rpm`: Is the machine running too fast?
    2.  `downtime_minutes`: Has it been stopping frequently recently?
    3.  `temperature_c`: Is it overheating?
    4.  `target_output`: Is the load too high?
*   **The Output**: A probability (0-100%) that acts as a **Risk Score**. We classify this into:
    *   üü¢ LOW (0-30%)
    *   üü° MEDIUM (30-50%)
    *   üü† HIGH (50-70%)
    *   üî¥ CRITICAL (>70%)

#### B. Supplier Delay Risk Model (`supplier_delay_rf_model.pkl`)
*   **Goal**: Predict if a material delivery will be late.
*   **The "Questions" (Features) the model asks:**
    1.  `supplier_id`: Who is the supplier? (Some are historically slower).
    2.  `material_type`: Is it a common or rare material?
    3.  `transportation_status`: Is it stuck in customs?
    4.  `order_quantity`: Is it a huge bulk order (harder to ship)?
*   **The Output**: Probability of delay.

---

## 2. Linear Regression (The "Trend Finder")

We use this for **Efficiency Prediction**.

### üéì Concept: How it Works
Imagine plotting points on a graph (Speed vs. Efficiency). Linear regression tries to draw the **best-fitting straight line** through these points.
*   It looks at the relationship between variables.
*   Example: "For every 10 degree rise in temperature, efficiency drops by 5%."
*   It gives you a precise number, not just a category.

### üõ†Ô∏è In Your Project:

#### Efficiency Prediction Model (`efficiency_lr_model.pkl`)
*   **Goal**: Predict the exact % efficiency of a machine.
*   **The "Questions" (Features) the model asks:**
    *   Same as production risk: Speed, Temperature, Downtime.
*   **The Output**: A continuous number (e.g., "87.5% Efficiency").
*   **Why Linear Regression?** Because efficiency is a continuous number, not a "Yes/No" category.

---

## üíª Tech Stack Code Explanation
In your `model_inference.py`, here is what's happening under the hood:
1.  **Loading**: We use `joblib` to load the saved `.pkl` "brains" of the models.
2.  **Preprocessing**: Machines don't understand text like "Supplier A". We use **Label Encoders** to turn "Supplier A" -> `1`, "Supplier B" -> `2`.
3.  **Inference**:
    *   `model.predict(data)`: Returns the final answer (e.g., "High Risk").
    *   `model.predict_proba(data)`: Returns the *confidence* (e.g., "I am 92% sure it is High Risk"). We use this for the risk score!

---

# üóÑÔ∏è Session 2: Database Architecture Pro-Course

Welcome to the Data Center! We are using **Supabase**, which is a supercharged version of **PostgreSQL**, the world's most advanced open-source relational database.

## üéì Concept: The Relational Database (SQL)
Think of a database like a giant, super-smart Excel workbook.
*   **Tables**: Like Excel sheets (e.g., "Production", "Suppliers").
*   **Columns (Fields)**: The specific data we track (e.g., "Temperature", "ID").
*   **Rows (Records)**: Individual entries (e.g., "Data from Machine 1 at 10:00 AM").
*   **Primary Key**: A unique ID for every single row (like a Social Security Number for data).

## üõ†Ô∏è Your Project's Data Schema
Your system relies on two main tables that capture the heartbeat of the factory.

### 1. `production_data` Table
This tracks the real-time health of your machines.

| Column Name | Type | Description |
| :--- | :--- | :--- |
| `id` | BIGINT | **Primary Key**. Unique ID for every record. |
| `timestamp` | TIMESTAMPTZ | The exact time the sensor read the data. Essential for time-series charts. |
| `machine_id` | TEXT | Which machine is this? (e.g., "M1", "Loom-Alpha"). |
| `target_output` | INTEGER | How much *should* it produce? |
| `actual_output`| INTEGER | How much *did* it produce? |
| `speed_rpm` | INTEGER | Application speed (Rotations Per Minute). |
| `temperature_c`| FLOAT | Engine temperature in Celsius. |
| `downtime_minutes`| FLOAT | How long it stopped in this cycle. |

### 2. `supplier_data` Table
This tracks the external logistics chain.

| Column Name | Type | Description |
| :--- | :--- | :--- |
| `id` | BIGINT | **Primary Key**. |
| `supplier_id` | TEXT | Who are we buying from? |
| `material_type`| TEXT | Cotton, Polyester, Dye, etc. |
| `expected_delivery_date` | DATE | When they *promised* it would arrive. |
| `actual_delivery_date` | DATE | When it *actually* arrived (used to calculate delays). |
| `transportation_status` | TEXT | "In Transit", "Customs", "Delivered". Critical for the risk model. |

## üîÑ The Data Flow Cycle
1.  **Generation**: Your Python scripts (`simulate_all.py`) act as "Virtual Sensors", generating fake realistic data.
2.  **Ingestion**: This data is sent to **Supabase** via the Internet.
3.  **Storage**: Supabase stores it securely in `production_data` and `supplier_data`.
4.  **Consumption**: Your Dashboard (`dashboard.py`) asks Supabase: *"Give me the last 100 records ordered by time!"*
5.  **Intelligence**: The Dashboard takes this raw data, feeds it to the **ML Models**, gets the predictions, and shows you the insights!

---
*End of Class. You are now ready to explain the logic and brain of your Smart Textile System!*
