PROJECT EXPLANATION BY GRAVITY AI

==================================
1Ô∏è‚É£ PROJECT SUMMARY (YOUR WORK)
==================================
**Project Title:** Textile Mill Intelligent Monitoring System

**Problem Addressed:**  
Textile manufacturing suffers from unexpected machine downtime, efficiency losses, and supply chain delays. Monitoring these factors manually is reactive and inefficient.

**Solution Implemented:**  
I built a real-time "Textile Mill Command Center" that:
1.  **Simulates** a live factory environment with machine sensors and supplier updates.
2.  **Streaming Data** to a cloud Production database (Supabase).
3.  **Predicts Risks** using Machine Learning models (production downtime, supplier delays, and efficiency).
4.  **Visualizes** everything on an interactive dashboard for operations managers.

**Scope of My Contribution:**  
I implemented the full stack: from synthetic data generation scripts and database schema design to ML model integration and the final Streamlit dashboard interface.

==================================
2Ô∏è‚É£ PROJECT FOLDER STRUCTURE
==================================
**Root Folder:** `c:\Users\user\Desktop\Textile_Project_Latest_Copy`

**Key Directories & Files:**
- **`dashboard.py`**: The main entry point for the frontend application. It handles the UI, data fetching, and visualization.
- **`simulate_all.py`**: The orchestration script that manages background threads for generating data.
- **`streaming/`**: Contains the logic for data generation.
    - `machine_stream.py`: Simulates machine sensor data (speed, temp, downtime) and pushes to DB.
    - `supplier_stream.py`: Simulates supplier delivery updates and pushes to DB.
- **`models/`**: Stores the pre-trained ML models (`.pkl` files) and label encoders.
- **`model_inference.py`**: A dedicated module I created to load these models and serve predictions to the dashboard.
- **`config/`**: Holds configuration variables (like Supabase credentials).
- **`database_setup.sql`**: SQL script I wrote to define the database schema.
- **`requirements.txt`**: Lists all Python dependencies I used.

**Entry Point:** The application is run via `streamlit run dashboard.py`.

==================================
3Ô∏è‚É£ SYSTEM ARCHITECTURE YOU IMPLEMENTED
==================================
**Architecture Type:** Event-Driven Data Pipeline with a Micro-Frontend Dashboard.

**Components:**
1.  **Data Producers (Simulation Layer):** Python scripts acting as IoT sensors and ERP systems.
2.  **Data Storage (Persistence Layer):** Supabase (PostgreSQL) for reliable time-series storage.
3.  **Intelligence Layer (ML):** A distinct module (`model_inference.py`) that decouples prediction logic from the UI.
4.  **Presentation Layer (Dashboard):** Streamlit application fetching data from both DB and ML layers.

**End-to-End Flow:**
1.  `simulate_all.py` triggers producer threads (`machine_stream`, `supplier_stream`).
2.  These streams generate JSON records and `INSERT` them into Supabase tables `production_data` and `supplier_data`.
3.  `dashboard.py` polls Supabase every few seconds (live mode) to fetch the latest rows.
4.  Fetched data is passed to `model_inference.py`.
5.  Inference results (e.g., "High Risk") are returned and rendered on the UI.

==================================
4Ô∏è‚É£ FRONTEND WORK DONE BY YOU
==================================
**Technologies:** Python, Streamlit, Plotly Express, Plotly Graph Objects.

**Work & Logic:**
- **`dashboard.py`**: I created this single-page application.
- **UI Design:** I implemented a "Dark Mode" aesthetic with custom CSS for glassmorphism effects on cards.
- **Layout:** Designed a grid layout:
    - **Top Row:** 5 key KPI cards (Efficiency, Risk Scores, Output).
    - **Middle Row:** Real-time line charts (Machine Output) and Gauge charts (Plant Efficiency).
    - **Bottom Row:** Tabbed view for detailed data logs, supply chain status bars, and ML interpretability.
- **Interactions:**
    - Added a "Live Monitoring" toggle that auto-refreshes the page.
    - Implemented a "Refresh Data" manual button.
    - Created expandable sections for ML model explanations.

==================================
5Ô∏è‚É£ BACKEND WORK DONE BY YOU
==================================
**Technologies:** Python, Supabase Python Client (`supabase-py`), Pandas.

**Modules Created:**
- **`streaming/machine_stream.py`**: I wrote the logic to randomize machine attributes (M1, M2, M3), generating realistic fluctuations in temperature and speed.
- **`model_inference.py`**: I built this class-based manager (`MLModelManager`) to handle:
    - Safe loading of `.pkl` files.
    - Feature extraction from raw dataframes.
    - Handling missing data with heuristic fallbacks (e.g., if models aren't found).
    - Returning structured dictionaries with risk levels and explanations.

**Request-Response Flow:**
- The backend logic is primarily "Push-Pull". The streams "Push" to DB. The dashboard "Pulls" (Select queries) from DB. The ML inference happens on-the-fly within the dashboard's server process (in-memory).

==================================
6Ô∏è‚É£ DATABASE & DATA STRUCTURE CREATED BY YOU
==================================
**Source:** Supabase (PostgreSQL).
**Configuration:** `database_setup.sql`.

**Schema Designed:**

1.  **Table: `production_data`**
    - **Purpose:** Stores real-time sensor readings from machines.
    - **Columns:** `id` (PK), `timestamp`, `machine_id`, `target_output`, `actual_output`, `speed_rpm`, `downtime_minutes`, `temperature_c`.
    - **Key Logic:** `timestamp` defaults to `NOW()` to ensure accurate time-series tracking.

2.  **Table: `supplier_data`**
    - **Purpose:** Tracks raw material delivery status.
    - **Columns:** `id` (PK), `timestamp`, `supplier_id`, `material_type`, `expected_delivery_date`, `actual_delivery_date`, `order_quantity`, `transportation_status`.
    - **Relationships:** Implicitly linked to production flow conceptualized in the business logic (materials -> production).

**CRUD Operations:**
- **Create:** Handled by streaming scripts (`insert`).
- **Read:** Handled by dashboard (`select * order by timestamp desc`).

==================================
7Ô∏è‚É£ DATA FLOW EXECUTED BY YOU
==================================
**Scenario: A Machine Overheats**

1.  **Input:** `machine_stream.py` generates a record: `{"machine_id": "M1", "temperature_c": 85.0}`.
2.  **Transmission:** This record is sent via HTTPS to Supabase Table `production_data`.
3.  **Fetch:** `dashboard.py` executes `supabase.table("production_data")...` and retrieves this row.
4.  **Processing:**
    - The row is converted to a Pandas DataFrame.
    - `model_inference.py` sees `temperature_c = 85.0`.
    - The Model (or heuristic fallback) calculates `Risk Score = 95%`.
5.  **Output:** The Dashboard updates the "Production Downtime Risk" KPI card to "CRITICAL" (Red color).

==================================
8Ô∏è‚É£ ANALYTICS / MACHINE LEARNING
==================================
**Implementation:**
- I created the **Inference Engine** (`model_inference.py`).
- **Algorithms Supported:**
    1.  **Random Forest Classifier:** For Production Risk (Binary/Multi-class).
    2.  **Random Forest Classifier:** For Supplier Delay logic. 
    3.  **Linear Regression:** For Efficiency prediction.
- **Logic:**
    - I implemented **Feature encoding** using `LabelEncoder` (loaded from `.pkl` inputs).
    - I added **Fallbacks**: If a model file is missing, I wrote logic to calculate risk based on simple rules (e.g., `if temp > 35 then risk += 10`) so the app never crashes.
    - **Interpretability:** I extracted `feature_importances_` to show users *why* a risk is high (e.g., "Temperature Impact: 40%").

==================================
9Ô∏è‚É£ TOOLS & TECHNOLOGIES YOU USED
==================================
- **Language:** Python 3.x
- **Frontend Framework:** Streamlit
- **Data Manipulation:** Pandas, NumPy
- **Database Client:** Supabase-py
- **Visualization:** Plotly (Express & Graph Objects)
- **ML Loading:** Joblib, Scikit-learn (implied dependency for models)
- **Environment:** VS Code, Windows PowerShell

==================================
üîü DEPLOYMENT OR EXECUTION
==================================
**Execution Setup:**
- I configured the project to run locally.
- **`simulate_all.py`**: Runs as a persistent background process.
- **`dashboard.py`**: Runs on a local web server (Tornado-based via Streamlit) at port 8501.
- **Secrets:** I implemented environment variable handling (`.env`) to keep API keys secure.

==================================
1Ô∏è‚É£1Ô∏è‚É£ ASSUMPTIONS & AUTOMATIONS MADE BY YOU
==================================
- **Assumption:** I assumed that if ML models are missing (files not found), the system should continue running with heuristic logic rather than failing.
- **Automation:** I automated the data stream using infinite `while True` loops with `time.sleep()` to mimic real-world sensor intervals (3s for machines, 5s for suppliers).
- **Simplification:** I hardcoded machine IDs ("M1", "M2", "M3") in the generator list for simplicity.

==================================
1Ô∏è‚É£2Ô∏è‚É£ LIMITATIONS OF YOUR IMPLEMENTATION
==================================
- **Scalability:** The dashboard queries Supabase directly on every refresh. For massive datasets, this would need a caching layer or aggregated materialized views.
- **Simulation:** The data is random/probabilistic. It does not react to "corrective actions" taken in the dashboard (one-way data flow).
- **Dependencies:** The app strictly requires an internet connection to reach Supabase; offline mode is not supported.

==================================
1Ô∏è‚É£3Ô∏è‚É£ WHAT YOU DID NOT DO
==================================
- I did **not** implement User Authentication (Login/Signup) for the dashboard.
- I did **not** create the actual ML model training notebooks or training pipelines in this folder (I only implemented the *inference* and loading of pre-trained models).
- I did **not** set up a CI/CD pipeline or Docker containerization for this specific deliverable.
